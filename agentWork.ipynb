{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# -------- Installing Dependencies --------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install psycopg2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install streamlit streamlit-apexjs streamlit-card streamlit-elements streamlit-option-menu streamlit-pandas-profiling Markdown markdown-it-py MarkupSafe matplotlib numerize scikit-learn seaborn pandas-profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------- Imports ----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import os\n",
    "import requests\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import json\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import psycopg2\n",
    "import streamlit as st\n",
    "from streamlit_option_menu import option_menu\n",
    "from numerize.numerize import numerize\n",
    "from datetime import timedelta\n",
    "import subprocess\n",
    "import time\n",
    "import concurrent.futures\n",
    "\n",
    "# Loading the env variables\n",
    "load_dotenv()\n",
    "db_name=os.getenv(\"DB\")\n",
    "db_user=os.getenv(\"DB_USER\")\n",
    "db_pwd=os.getenv(\"DB_PASSWORD\")\n",
    "db_host = os.getenv('DB_HOST')\n",
    "db_port = os.getenv('DB_PORT')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## --------- ETL PROCESS ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Excécute  ETL PROCESS\n",
    "import subprocess\n",
    "\n",
    "def execute_batch_file(batch_file_path):\n",
    "    try:\n",
    "        batch_command = f\"cmd /c {batch_file_path}\"\n",
    "        \n",
    "        process = subprocess.Popen(batch_command, shell=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        stdout, stderr = process.communicate()\n",
    "\n",
    "        print(\"Sortie standard:\")\n",
    "        print(stdout.decode())\n",
    "        \n",
    "        if stderr:\n",
    "            print(\"Erreurs:\")\n",
    "            print(stderr.decode())\n",
    "        \n",
    "        # Vérification du code de sortie\n",
    "        if process.returncode == 0:\n",
    "            print(\"Le fichier batch a été exécuté avec succès.\")\n",
    "        else:\n",
    "            print(f\"Erreur lors de l'exécution du fichier batch. Code de sortie : {process.returncode}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Une erreur s'est produite : {e}\")\n",
    "\n",
    "batch_file_path = r\"D:\\workspace\\BI_PROJECT\\build\\jAlimentationDB\\jAlimentationDB_run.bat\"\n",
    "execute_batch_file(batch_file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------- Agent Work ---------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Database connection \n",
    "\n",
    "try:\n",
    "    # Connect to the PostgreSQL database\n",
    "    conn = psycopg2.connect(f\"host={db_host} port={db_port} dbname={db_name} user={db_user} password={db_pwd} \"\n",
    "    )\n",
    "    if conn is not None:\n",
    "        print('> Database connection established ... ')\n",
    "    else:\n",
    "        print(\"> There is no  connection ! Check your connection prameters! \")\n",
    "    \n",
    "\n",
    "except (Exception) as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WORKER JOB\n",
    "\n",
    "def surveiller_seuils_ventes(connection, seuil_inferieur, seuil_superieur):\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            query = \"\"\"\n",
    "                SELECT COUNT(*)\n",
    "                FROM VENTE_DWH.FAIT_VENTE\n",
    "                WHERE NB_VENTE < %s OR NB_VENTE > %s\n",
    "            \"\"\"\n",
    "            cursor.execute(query, (seuil_inferieur, seuil_superieur))\n",
    "            count = cursor.fetchone()[0]\n",
    "            \n",
    "            if count > 0:\n",
    "                print(f\"Alerte : Il y a {count} enregistrements avec des ventes en dehors des seuils.\")\n",
    "            else:\n",
    "                print(\"Aucune alerte : Les ventes sont dans les limites attendues.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la surveillance des seuils de ventes : {e}\")\n",
    "\n",
    "def surveiller_performances_requetes(connection):\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            query = \"\"\"\n",
    "                SELECT query, total_time\n",
    "                FROM pg_stat_statements\n",
    "                ORDER BY total_time DESC\n",
    "                LIMIT 5\n",
    "            \"\"\"\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "            \n",
    "            print(\"Top 5 des requêtes les plus lentes :\")\n",
    "            for row in results:\n",
    "                print(f\"Requête : {row[0]}, Temps total d'exécution : {row[1]} ms\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors du suivi des performances des requêtes SQL : {e}\")\n",
    "\n",
    "def surveiller_modifications_schema(connection):\n",
    "    try:\n",
    "        with connection.cursor() as cursor:\n",
    "            query = \"\"\"\n",
    "                SELECT table_name, column_name, data_type\n",
    "                FROM information_schema.columns\n",
    "                WHERE table_schema = VENTE_DWH AND table_name = DIM_CLIENT\n",
    "            \"\"\"\n",
    "            cursor.execute(query)\n",
    "            results = cursor.fetchall()\n",
    "            \n",
    "            print(\"Modifications de schéma détectées :\")\n",
    "            for row in results:\n",
    "                print(f\"Table : {row[0]}, Colonne : {row[1]}, Type de données : {row[2]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la surveillance des modifications de schéma : {e}\")\n",
    "\n",
    "def mettre_a_jour_prix_produits(connection):\n",
    "    try:\n",
    "        with connection.cursor() as cur:\n",
    "            query = \"\"\"\n",
    "                UPDATE VENTE_DWH.DIM_PRODUIT\n",
    "                SET PRIX_ACHAT_PRODUIT = PRIX_ACHAT_PRODUIT * 1.05,\n",
    "                    PRIX_VENTE_PRODUIT = PRIX_VENTE_PRODUIT * 1.05\n",
    "                WHERE BL_LIGNE_ACTIVE = 1\n",
    "            \"\"\"\n",
    "            cur.execute(query)\n",
    "            print(cur)\n",
    "            connection.commit()\n",
    "            print(f\"{cur} prix de produits ont été mis à jour.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de la mise à jour des prix des produits : {e}\")\n",
    "if conn :\n",
    "    # Create a ThreadPoolExecutor\n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        # Submit the functions to the executor\n",
    "        futures = [\n",
    "            executor.submit(mettre_a_jour_prix_produits, conn),\n",
    "            executor.submit(surveiller_seuils_ventes, conn, 2, 10),\n",
    "            executor.submit(surveiller_modifications_schema, conn),\n",
    "            executor.submit(surveiller_performances_requetes, conn),\n",
    "        ]\n",
    "\n",
    "        # Wait for all tasks to complete\n",
    "        concurrent.futures.wait(futures)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "\n",
    "\n",
    "# Establish a connection to your PostgreSQL database\n",
    "\n",
    "\n",
    "db_name='dwh',\n",
    "db_user='postgres',\n",
    "db_pwd='postgres',\n",
    "db_host='127.0.0.1',\n",
    "db_port='5432' \n",
    "try:\n",
    "    conn = psycopg2.connect(f\"host={db_host} port={db_port} dbname={db_name} user={db_user} password={db_pwd} \")\n",
    "    cur = conn.cursor()\n",
    "    print(\"Connected to the database!\")\n",
    "    # Perform database operations here\n",
    "    query = '''SELECT \n",
    "\n",
    "    VILLE_CLIENT,\n",
    "    PAYS_CLIENT,\n",
    "    SUM(NB_VENTE) AS TotalSales\n",
    "    FROM \n",
    "        DIM_CLIENT \n",
    "    JOIN \n",
    "        FAIT_VENTE ON DIM_CLIENT.ID_DIM_CLIENT = FAIT_VENTE.ID_DIM_CLIENT\n",
    "    GROUP BY CUBE ( VILLE_CLIENT, PAYS_CLIENT)\n",
    "    ORDER BY ( VILLE_CLIENT, PAYS_CLIENT);'''\n",
    "\n",
    "    # cur.execute(query)\n",
    "\n",
    "    sales_data = pd.read_sql(query,conn)\n",
    "\n",
    "\n",
    "\n",
    "    # print(SALES_LOCATIONS_DIM)\n",
    "\n",
    "    \n",
    "\n",
    "    # Close the database connection\n",
    "    conn.close()\n",
    "\n",
    "    # Display the top 10 selling countries (pays)\n",
    "    lowest_10_pays = sales_data[sales_data['Pays'] != 'Total'].groupby('Pays')['TotalSales'].sum().nsmallest(10)\n",
    "    print(\"lowest 10 Selling Countries (Pays):\")\n",
    "    print(lowest_10_pays)\n",
    "    print(\"add more ads\")\n",
    "\n",
    "    # Display the top 10 selling cities (villes)\n",
    "    lowest_10_villes = sales_data[sales_data['City'] != 'Total'].groupby('City')['TotalSales'].sum().nsmallest(10)\n",
    "    print(\"\\n lowest 10 Selling Cities (Villes):\")\n",
    "    print(lowest_10_villes)\n",
    "    print(\"create promotions\")\n",
    "\n",
    "    # Display the top 10 best-selling products\n",
    "    top_10_products = sales_data[sales_data['ProductName'] != 'Total'].groupby('ProductName')['TotalSales'].sum().nlargest(10)\n",
    "    print(\"\\nTop 10 Best-Selling Products:\")\n",
    "    print(top_10_products)\n",
    "    print(\"stock more\") \n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fermer la connexion à la base de données\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
